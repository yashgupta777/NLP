{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nltk_Chunking.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/yashgupta777/NLP/blob/master/Nltk_Chunking.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "KrNsGCqNU3Ug",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "fc0f330d-8912-4c3f-e857-8c361c0cc06d"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "# Now that we know the parts of speech, we can do what is called chunking, and\n",
        "# group words into hopefully meaningful chunks. One of the main goals of chunking is to group into what are known as \"noun phrases.\" \n",
        "# These are phrases of one or more words that contain a noun, maybe some descriptive words, maybe a verb, and maybe something like an adverb. \n",
        "# The idea is to group nouns with the words that are in relation to them.\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "\n",
        "def process_content():\n",
        "    try:\n",
        "        for i in tokenized:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "            chunkParser = nltk.RegexpParser(chunkGram)\n",
        "            chunked = chunkParser.parse(tagged)\n",
        "            print(chunked)\n",
        "            chunked.draw()     \n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "process_content()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
            "  'S/POS\n",
            "  (Chunk ADDRESS/NNP)\n",
            "  BEFORE/IN\n",
            "  (Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
            "  OF/IN\n",
            "  (Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
            "  OF/IN\n",
            "  (Chunk THE/NNP UNION/NNP January/NNP)\n",
            "  31/CD\n",
            "  ,/,\n",
            "  2006/CD\n",
            "  (Chunk THE/NNP PRESIDENT/NNP)\n",
            "  :/:\n",
            "  (Chunk Thank/NNP)\n",
            "  you/PRP\n",
            "  all/DT\n",
            "  ./.)\n",
            "no display name and no $DISPLAY environment variable\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}